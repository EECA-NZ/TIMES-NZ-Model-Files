This document outlines the rules and frameworks for the `PREPARE-TIMES-NZ` module. 

We want to make sure everything is replicable and scalable while minimising complexity as much as feasible. Minimising complexity should be our key guiding principle.

1) All data transformations should be replicable, with all source inputs and scripts clearly traceable. 
2) Steps are broken down into clear stages. This means it is clear for future modellers exactly what should be happening and when. 
3) It should be really easy to add new data/methods/models/scenarios to future iterations TIMES-NZ. 

## Stages

The module is broken down into the following stages: 

(THIS LIST A WIP - MAY ADJUST AS WE GO DEPENDING ON WHAT MAKES SENSE)

0) System settings. Read configuration files and make any key parameters (such as base year) available for all future steps. 
1) Prepare raw data, transforming and tidying where necessary (ie applying tidy data principles to excel files)
2) Processing - prepare base year data files. Combinations, transformations, apply assumptions, etc.
3) Processing - prepare scenario files. Combinations, transformations, apply assumptions, etc.
4) Format the prepared data into files for Veda, using structures defined by the configuration settings and reading base year/scenario data as needed.

## data_raw

All raw data is stored in this directory. It remains untouched from how it was downloaded or otherwise accessed.

 - `eeca_data` - any data sourced internally, such as the eeud extract. 
 - `external_data` - any data sourced externally. This is further categorised by source institution:
    - `mbie`
    - `electricity_authority`
    - etc. We can add anything we like to this, but it's good to keep separated by institution. 
 - `archive` - the original TIMES 2.1.3 excel files and a copy of the raw_tables.txt generated from these files. 
 - `coded_assumptions` - any raw files where assumptions have been built in. These are usually files that have been manually produced in some way. They're broken down by subject area, eg:
    - `electricity_generation`    
 - `concordances` - these are manual files, similar to `coded_assumptions`, but specifically designed to map different categories together, like regions to islands, etc. 
    - It might make more sense to add this to a subcategory of `coded_assumptions`
 - `user_config` - these are the toml configuration files which will define the structure of the final excel file outputs. You can read more about these in `readme.md`


## data_intermediate

This directory is ignored by git and will not be tracked. To populate it, you will need to clone the repo and execute the module. 

The files in this module, depending on the stage, are either tidied raw data or tables used for other modules.

The structure of this has not yet been properly defined. It should most likely follow the same stages as the scripts ie 

0) settings
1) raw_data
2) base year 
3) scenarios 

and within these stages, be broken further down by subject area as needed (eg `residential_demand` etc)

NOTE: these stages are not currently structured as above and need a bit of tidying.

## output

Similar to `data_intermediate`, this directory is not tracked by git and is fully generated by the module.

It contains all output files in Veda format for use by the TIMES model generator. 

When this system is fully mature, we will retire this directory and instead output all excel files for Veda directly to the `TIMES-NZ` module, allowing the model to run. 









    
    