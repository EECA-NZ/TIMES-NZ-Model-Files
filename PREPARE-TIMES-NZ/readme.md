## Pre-processing with Python

```
THIS MODULE IS UNDER DEVELOPMENT
```

To prepare the TIMES-NZ model files, we will be implementing pre-processing using the following command:

```
python prepare_times_nz.py
```

This script will handle the necessary steps to preprocess the data and ensure it is ready for further analysis and modeling.

Currently, a one-off script (`scripts/read_summary.py`) was used to produce the csv layout mirroring the existing structure (using the XL2TIMES `raw_tables.txt` as source data).

Then, `scripts/prepare_times_nz.py` uses these files to generate the smaller excel files, which Veda can use to produce TIMES runs. 

While this version of the model uses autogenerated excel files, all original excel input Veda files are still kept in the repo (`PREPARE-TIMES-NZ/data_raw/archive`). The logic and source data in these sheets needs to be readily available to enable further development work.

### TO DO

In the future, the input csvs, including input data, assumptions, and satellite model methods, will need to properly sourced and documented. The intent is that this work will be done for TIMES 3.0. The logic currently contained in excel sheets will ideally be reviewed and codified in scripts. Input data handling can also be automated as part of this workflow, increasing tracability. 

## STRUCTURE

All inputs will be in `data_raw`. 

This includes:
 - ALL source data in csv form. 
 - All configuration settings 
 - Where we are using figures from research papers to modify or estimate shares etc, these can just have sources linked 

The raw data in `data_raw` should be the single source of truth.

This module should also contain the main methodology documentation, which describes sources and topology for each step of the model buildup. If we estimate demand disaggregations or project demand per sectors, the data and scripts for this will be here, and there will be an entry in the methodology document. This module, then, is the single source of truth for all technical information on TIMES-NZ. 


`data_intermediate` stores the files in structures aligning to the final outputs. These are csv files ready to be converted to Veda tags. This area is currently slightly loose, and we may need to break it up further (possibly by stages, as we progress we will learn more about how to do this)



`output` stores the outputs which are the excel files to be read by Veda (and later XL2TIMES, hopefully). 



## CURRENT PLAN 

All subject areas get their own input folder and scripts. These can be tied to 








## General Future State

```mermaid
flowchart LR
    CSV[("CSV/TOML Files")]
    EXCEL["Excel Processing"]
    MIGRATE_FORMULAS["Migrate formula logic (iterative)"]
    VEDA["VEDA Analysis"]
    OUTPUT["Data Output"]
    SHINY["Public Shiny Dashboard"]
    
    INTERNAL_QA["Internal QA Tools"]    

    subgraph XL2TIMES["XL2Times"]
        EXCEL["Excel Processing"]
        VEDA["VEDA Analysis"]
    end



    CSV --> EXCEL
    EXCEL --> MIGRATE_FORMULAS
    MIGRATE_FORMULAS --> CSV
    EXCEL --> VEDA
    VEDA --> OUTPUT
    OUTPUT --> SHINY
    OUTPUT --> INTERNAL_QA
    


```
